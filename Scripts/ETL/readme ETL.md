Processo ETL
ğŸ“Œ VisÃ£o Geral

O processo de ETL (Extract, Transform, Load) foi desenvolvido para simular um pipeline de dados utilizado em ambientes reais de Business Intelligence no varejo.

O pipeline tem como objetivo garantir:

PadronizaÃ§Ã£o dos dados

Qualidade e consistÃªncia das informaÃ§Ãµes

AplicaÃ§Ã£o de regras de negÃ³cio

PreparaÃ§Ã£o dos dados para consumo analÃ­tico

ğŸ—ï¸ Arquitetura do Pipeline

O fluxo de dados segue a seguinte estrutura:

RAW â†’ STAGING â†’ BI

ğŸ”¹ RAW

Camada responsÃ¡vel por armazenar os dados brutos, simulando extraÃ§Ãµes de sistemas transacionais e e-commerce.

CaracterÃ­sticas:

Dados sem tratamento

Possibilidade de valores nulos

InconsistÃªncias de formataÃ§Ã£o

PreservaÃ§Ã£o da estrutura original da fonte

ğŸ”¹ STAGING

Camada intermediÃ¡ria onde ocorre todo o tratamento e validaÃ§Ã£o dos dados.

Nesta etapa sÃ£o aplicadas:

Limpezas estruturais

PadronizaÃ§Ãµes textuais

Tratamento de valores nulos

AplicaÃ§Ã£o de regras de negÃ³cio

Enriquecimento analÃ­tico

ğŸ§© ExtraÃ§Ã£o dos Dados

A extraÃ§Ã£o foi simulada atravÃ©s da leitura de arquivos CSV armazenados na camada RAW.

Principais etapas:

Leitura dos arquivos utilizando Pandas

NormalizaÃ§Ã£o dos nomes das colunas

PreparaÃ§Ã£o dos dados para transformaÃ§Ã£o

Exemplo das operaÃ§Ãµes realizadas:

ConversÃ£o de colunas para minÃºsculo

RemoÃ§Ã£o de espaÃ§os em branco

Garantia de consistÃªncia estrutural

ğŸ”§ TransformaÃ§Ãµes Aplicadas

Cada tabela passou por transformaÃ§Ãµes especÃ­ficas baseadas em regras de negÃ³cio e boas prÃ¡ticas de modelagem dimensional.

ğŸ‘¤ DimensÃ£o Customer

TransformaÃ§Ãµes realizadas:

PadronizaÃ§Ã£o de textos

Tratamento de valores nulos

ValidaÃ§Ã£o de atributos demogrÃ¡ficos

RemoÃ§Ã£o de duplicidades

Garantia de consistÃªncia nos identificadores de cliente

Objetivo:

Garantir uma base confiÃ¡vel para anÃ¡lise de comportamento do consumidor.

ğŸ›ï¸ DimensÃ£o Product

TransformaÃ§Ãµes realizadas:

PadronizaÃ§Ã£o de nomes de produtos e marcas

ConversÃ£o de preÃ§os para formato numÃ©rico

ExclusÃ£o de valores invÃ¡lidos ou negativos

Mapeamento de subcategorias baseado em domÃ­nio de negÃ³cio

ValidaÃ§Ã£o hierÃ¡rquica entre categoria e subcategoria

Tratamento de subcategorias nÃ£o classificadas

Objetivo:

Criar uma estrutura categÃ³rica confiÃ¡vel para anÃ¡lise de mix de produtos e performance comercial.

ğŸ¬ DimensÃ£o Store

TransformaÃ§Ãµes realizadas:

PadronizaÃ§Ã£o de nomes de lojas

Mapeamento de regiÃµes para estados vÃ¡lidos

AssociaÃ§Ã£o de estados Ã s respectivas capitais

ExclusÃ£o de registros sem localizaÃ§Ã£o vÃ¡lida

PadronizaÃ§Ã£o de nomenclaturas geogrÃ¡ficas

Objetivo:

Permitir anÃ¡lises regionais e geogrÃ¡ficas consistentes.

ğŸ“… DimensÃ£o Date

TransformaÃ§Ãµes realizadas:

CriaÃ§Ã£o de chaves substitutas baseadas na data

ExtraÃ§Ã£o de atributos temporais

InclusÃ£o de indicadores analÃ­ticos como:

Nome do mÃªs

Trimestre

Dia da semana

IdentificaÃ§Ã£o de finais de semana

NÃºmero da semana do ano

Objetivo:

Facilitar anÃ¡lises temporais e identificaÃ§Ã£o de sazonalidades.

ğŸ’° Tabela Fato Sales

TransformaÃ§Ãµes realizadas:

RemoÃ§Ã£o de registros com valores nulos

ValidaÃ§Ã£o de mÃ©tricas financeiras

Garantia de integridade referencial com dimensÃµes

Monitoramento e registro da quantidade de valores descartados

PadronizaÃ§Ã£o de tipos numÃ©ricos

Objetivo:

Garantir confiabilidade nas mÃ©tricas utilizadas nos dashboards.

ğŸ§ª Qualidade dos Dados

Durante o processo ETL foram implementadas verificaÃ§Ãµes para garantir integridade e consistÃªncia dos dados, incluindo:

ValidaÃ§Ã£o de tipos de dados

ExclusÃ£o de registros invÃ¡lidos

Monitoramento de valores nulos

PadronizaÃ§Ã£o textual

RemoÃ§Ã£o de duplicidades

âš™ï¸ OrquestraÃ§Ã£o do Pipeline

Todas as transformaÃ§Ãµes sÃ£o executadas por uma funÃ§Ã£o central responsÃ¡vel por coordenar o fluxo ETL.

Essa funÃ§Ã£o executa as transformaÃ§Ãµes de forma sequencial, garantindo que todas as dimensÃµes e tabelas fato sejam processadas corretamente antes do consumo analÃ­tico.

ğŸ’¾ Carregamento dos Dados

ApÃ³s o tratamento, os dados sÃ£o exportados para a camada STAGING, onde ficam disponÃ­veis para consumo em ferramentas de Business Intelligence.

Formato de saÃ­da:

Arquivos CSV estruturados

Dados prontos para modelagem e visualizaÃ§Ã£o

ğŸ“Š BenefÃ­cios do Processo

O pipeline desenvolvido permite:

SimulaÃ§Ã£o de ambiente corporativo real

Melhoria na confiabilidade dos dados

Base estruturada para anÃ¡lises estratÃ©gicas

Escalabilidade para inclusÃ£o de novas fontes

Durante o desenvolvimento do dashboard no Tableau, foi realizada uma etapa de normalizaÃ§Ã£o e padronizaÃ§Ã£o das colunas mÃ©tricas para garantir consistÃªncia e confiabilidade nas anÃ¡lises.

Apesar dos dados jÃ¡ estarem estruturados em banco e exportados para CSV, algumas colunas numÃ©ricas foram reconhecidas pelo Tableau como texto, o que poderia comprometer cÃ¡lculos e indicadores. Para resolver esse cenÃ¡rio, foram criados campos calculados aplicando funÃ§Ãµes de limpeza e conversÃ£o de tipo, como TRIM() para remover possÃ­veis espaÃ§os ocultos e FLOAT() para garantir o correto tratamento numÃ©rico.

Essa abordagem permitiu padronizar mÃ©tricas como receita, custo e valores financeiros, assegurando que os KPIs fossem calculados corretamente. AlÃ©m disso, foi feita uma validaÃ§Ã£o criteriosa para identificar quais colunas realmente necessitavam de tratamento, evitando transformaÃ§Ãµes desnecessÃ¡rias e mantendo a eficiÃªncia do modelo analÃ­tico.

Esse processo reforÃ§a a importÃ¢ncia da qualidade e governanÃ§a dos dados como etapa fundamental antes da construÃ§Ã£o de visualizaÃ§Ãµes e geraÃ§Ã£o de insights para o negÃ³cio.